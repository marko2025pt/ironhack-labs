{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cfe255d",
   "metadata": {},
   "source": [
    "### Step 1: Loading OpenAI API Key from `.env`\n",
    "\n",
    "In this cell, we securely load the OpenAI API key from a `.env` file instead of hardcoding it in the notebook.  \n",
    "\n",
    "**Steps performed:**\n",
    "1. Import required libraries: `os` for environment variables, `dotenv` to load `.env` files, and `openai` for API access.\n",
    "2. Load environment variables from the `.env` file using `load_dotenv()`.\n",
    "3. Set the OpenAI API key for the `openai` library with `os.getenv(\"OPENAI_API_KEY\")`.\n",
    "4. Optionally, verify that the key is loaded by printing `True` if it exists.\n",
    "\n",
    "**Security Note:** This prevents the API key from being pushed to GitHub if `.env` is added to `.gitignore`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a69b1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded: True\n"
     ]
    }
   ],
   "source": [
    "# Load OpenAI API key from .env\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Verify the key is loaded (optional)\n",
    "print(\"API Key loaded:\", bool(openai.api_key))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee90679a",
   "metadata": {},
   "source": [
    "### Step 2: Prep audio (manually)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26f79de",
   "metadata": {},
   "source": [
    "### Step 3: Basic Transcription (Without Chunking and Prompt)\n",
    "\n",
    "In this step, we transcribe a short audio file using OpenAI's Whisper API. \n",
    "We will:\n",
    "- Load the audio file\n",
    "- Send it to the Whisper API\n",
    "- Receive the transcription\n",
    "- Display the text output\n",
    "\n",
    "This helps us understand the API response format before handling longer audio files or chunking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e18ab1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:\n",
      "\n",
      "It was rather interesting just to watch them gathering their materials and bouncing around their, what they call it, kangaroo walk or something like that. Who named it that? I don't know. I bet those men are going to get quite a reception when they get back to Maine. Oh, yes. I'll be so glad when they land back now. But I think that's pretty well a fact, because they've landed so many safely now that I feel relieved. Just getting off of the moon was the thing that was. Have they met with the one that was circling? Yes, they've rendezvoused. So I understand. That wasn't shown either, but they say they have rendezvoused. So that's a matter of making the circles and then coming down. What do you sort of imagine for the future? Do you imagine them sending up ships? I think they will. I think they will do some more exploring up there. Very positive. Because that was such a very small area, when you think of it, that they just gathered rocks and samples of soil and all. And they did probe for some samples. And just what was going to come of that, I don't know. I'll be glad to read it.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Basic Transcription (Without Chunking and without Prompt)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to audio file\n",
    "audio_file_path = Path(\"audio/CA138clip.mp3\")\n",
    "\n",
    "# Open the audio file in binary mode\n",
    "with open(audio_file_path, \"rb\") as audio_file:\n",
    "    # Call Whisper API for transcription\n",
    "    transcript = openai.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",\n",
    "        file=audio_file\n",
    "    )\n",
    "\n",
    "# Display the transcribed text\n",
    "print(\"Transcription:\\n\")\n",
    "print(transcript.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f879a2",
   "metadata": {},
   "source": [
    "### Step 4: Transcription with Prompts (Guided Approach)\n",
    "\n",
    "In this step, we guide the Whisper API transcription by providing a **prompt** with context.  \n",
    "\n",
    "**Objectives:**\n",
    "- Improve transcription accuracy for technical terms, names, or specific context.\n",
    "- Compare results with the unguided transcription from Step 2.\n",
    "- Understand how Whisper handles contextual guidance.\n",
    "\n",
    "**Process:**\n",
    "1. Define a prompt describing the context (e.g., meeting type, technical terms).\n",
    "2. Send both the audio file and prompt to Whisper API.\n",
    "3. Receive and display the guided transcription.\n",
    "\n",
    "\n",
    "### NOTE\n",
    "\n",
    "In this step, we guide the Whisper API transcription using a **prompt specifically adapted** to the content of the audio.  \n",
    "\n",
    "Since this audio is an interview after the first moon landing, the prompt reflects the context:\n",
    "- Historical interview about astronauts\n",
    "- Technical and mission details\n",
    "- Dialogue flow between interviewer and interviewee\n",
    "\n",
    "This ensures that the transcription is accurate, clear, and suitable for archival purposes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be810473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guided Transcription:\n",
      "\n",
      "It was rather interesting just to watch them gathering their materials and bouncing around. What do they call it? Kangaroo walk? Something like that. Who named it that? I don't know. I bet those men are going to get quite a reception when they get back to Earth. Oh, yes. I'll be so glad when they land back now. I think that's pretty well a fact because they've landed so many safely now that I feel relieved. Just getting off of the moon was the thing that was... Have they met with the one that was circling? Yes, they've rendezvoused, so I understand. That wasn't shown either, but they say they have rendezvoused. That's a matter of making the circles and then coming down. What do you sort of imagine for the future? Do you imagine them standing up? I think they will. I think they will do some more exploring up there. Very positive. Because that was such a very small area, when you think of it, that they just gathered rocks and samples of soil and all. They did probe for some samples and just what was going to come of that. I don't know. I'd be glad to read it.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Guided Transcription with Adapted Prompt\n",
    "\n",
    "# Adapted prompt for the actual audio content\n",
    "prompt_text = (\n",
    "    \"You are transcribing a historical interview about astronauts returning from the first moon landing. \"\n",
    "    \"Focus on capturing names, mission details, technical terms, and any dialogue accurately. \"\n",
    "    \"Preserve the flow of conversation and clearly represent questions and answers. \"\n",
    "    \"The transcription should be clear, professional, and suitable for archival or historical records.\"\n",
    ")\n",
    "\n",
    "# Open the audio file in binary mode\n",
    "with open(audio_file_path, \"rb\") as audio_file:\n",
    "    # Call Whisper API with the adapted prompt\n",
    "    guided_transcript = openai.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",\n",
    "        file=audio_file,\n",
    "        prompt=prompt_text\n",
    "    )\n",
    "\n",
    "# Display the guided transcription text\n",
    "print(\"Guided Transcription:\\n\")\n",
    "print(guided_transcript.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590194d1",
   "metadata": {},
   "source": [
    "### Step 5: Audio Chunking\n",
    "\n",
    "For long recordings, we split the audio into smaller, manageable chunks for transcription.  \n",
    "\n",
    "**Objectives:**\n",
    "- Split audio into segments\n",
    "- Make it easier to process with Whisper\n",
    "- Preserve timestamps for combining results later\n",
    "\n",
    "**NOTE:**\n",
    "We used a small audio file (1:26)\n",
    "We split the audio into 20-second chunks to demonstrate chunking, even for a short clip. The same technique can be used for largers audio file.  \n",
    "This allows us to show multiple chunks in the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741c14a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio loaded successfully! Duration: 86.20 seconds\n",
      "Created chunk 1: audio_chunks\\chunk_1.mp3 (20.00 seconds)\n",
      "Created chunk 2: audio_chunks\\chunk_2.mp3 (20.00 seconds)\n",
      "Created chunk 3: audio_chunks\\chunk_3.mp3 (20.00 seconds)\n",
      "Created chunk 4: audio_chunks\\chunk_4.mp3 (20.00 seconds)\n",
      "Created chunk 5: audio_chunks\\chunk_5.mp3 (6.20 seconds)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Audio Chunking\n",
    "\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# === Step 1: Set audio file path (relative to notebook) ===\n",
    "audio_file_path = Path(\"audio/CA138clip.mp3\")\n",
    "\n",
    "# === Step 2: Load audio ===\n",
    "audio = AudioSegment.from_file(audio_file_path)\n",
    "print(f\"Audio loaded successfully! Duration: {len(audio)/1000:.2f} seconds\")\n",
    "\n",
    "# === Step 3: Prepare output folder (relative) ===\n",
    "output_dir = Path(\"audio_chunks\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# === Step 4: Split audio into 20-second chunks ===\n",
    "chunk_length_ms = 20 * 1000  # 20 seconds\n",
    "chunks = []\n",
    "\n",
    "for i, start in enumerate(range(0, len(audio), chunk_length_ms)):\n",
    "    end = min(start + chunk_length_ms, len(audio))\n",
    "    chunk = audio[start:end]\n",
    "    chunk_filename = output_dir / f\"chunk_{i+1}.mp3\"\n",
    "    chunk.export(chunk_filename, format=\"mp3\")\n",
    "    chunks.append(chunk_filename)\n",
    "    print(f\"Created chunk {i+1}: {chunk_filename} ({len(chunk)/1000:.2f} seconds)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267e0041",
   "metadata": {},
   "source": [
    "### Step 6: Transcribing Audio Chunks with Timestamps\n",
    "\n",
    "We transcribe each audio chunk individually and adjust timestamps to match the position in the original audio.\n",
    "\n",
    "- Each chunk is sent to the OpenAI Whisper API.\n",
    "- Transcription results include text and start/end times.\n",
    "- Chunks are combined into a full transcript.\n",
    "- The final output includes timestamps for each segment for easy navigation and search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5065453a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing audio_chunks\\chunk_1.mp3...\n",
      "Transcribing audio_chunks\\chunk_2.mp3...\n",
      "Transcribing audio_chunks\\chunk_3.mp3...\n",
      "Transcribing audio_chunks\\chunk_4.mp3...\n",
      "Transcribing audio_chunks\\chunk_5.mp3...\n",
      "[0.0s - 20.0s]: It was rather interesting just to watch them gathering their materials and bouncing around. What do they call it? Kangaroo walk? Really? Something like that. Who named it that? I don't know. I bet those men are going to get quite a reception when they get back. Oh yes, I'll be so glad.\n",
      "\n",
      "[20.0s - 40.0s]: I think that's pretty well fact, because they've landed so many safely now that I feel relieved. Just getting off of the moon was the thing that was... Have they met with the one that was circling the moon? Yes, they've round the moon.\n",
      "\n",
      "[40.0s - 60.0s]: So I understand. That wasn't shown either, so I... But they say they have rendezvoused, so... That's a matter of making the circles and then coming down. What do you sort of imagine for the future? Do you imagine them standing up? I think they will. I think they will.\n",
      "\n",
      "[60.0s - 80.0s]: We'll do some more exploring up there. Very positive. Because that was such a very small area, when you think of it, that they just gathered rocks and samples of soil and all. And they did a probe.\n",
      "\n",
      "[80.0s - 86.2s]: For more information, visit www.FEMA.gov\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Transcribing Audio Chunks with Timestamps\n",
    "\n",
    "chunk_files = sorted(Path(\"audio_chunks\").glob(\"chunk_*.mp3\"))\n",
    "\n",
    "full_transcript = []\n",
    "current_offset_ms = 0  # Keep track of chunk start\n",
    "\n",
    "for chunk_file in chunk_files:\n",
    "    print(f\"Transcribing {chunk_file}...\")\n",
    "\n",
    "    with open(chunk_file, \"rb\") as audio:\n",
    "        response = openai.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio,\n",
    "            prompt=\"This is an interview after the moon landing. Transcribe accurately.\"\n",
    "        )\n",
    "    \n",
    "    text = response.text\n",
    "\n",
    "    start_sec = current_offset_ms / 1000\n",
    "    end_sec = start_sec + (len(AudioSegment.from_file(chunk_file)) / 1000)\n",
    "\n",
    "    full_transcript.append({\n",
    "        \"start\": start_sec,\n",
    "        \"end\": end_sec,\n",
    "        \"text\": text\n",
    "    })\n",
    "\n",
    "    current_offset_ms += len(AudioSegment.from_file(chunk_file))\n",
    "\n",
    "# Print full transcript with timestamps\n",
    "for segment in full_transcript:\n",
    "    print(f\"[{segment['start']:.1f}s - {segment['end']:.1f}s]: {segment['text']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d367cb",
   "metadata": {},
   "source": [
    "### Step 7: Exporting Transcriptions with Timestamps\n",
    "\n",
    "In this step, we take the chunked transcription data and export it in multiple formats:\n",
    "\n",
    "1. **Human-readable text file** - easy to read and review\n",
    "2. **SRT (subtitle) file** - can be used for video subtitles\n",
    "3. **JSON file** - structured format for programmatic use\n",
    "\n",
    "Each format includes timestamps for each chunk to preserve context and sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35cba26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text file exported to transcriptions\\transcription.txt\n",
      "SRT file exported to transcriptions\\transcription.srt\n",
      "JSON file exported to transcriptions\\transcription.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Example transcription data (replace with your actual chunked results)\n",
    "transcriptions = [\n",
    "    {\"start\": 0.0, \"end\": 20.0, \"text\": \"It was rather interesting just to watch them gathering their materials and bouncing around. What do they call it? Kangaroo walk? Really? Something like that. Who named it that? I don't know. I bet those men are going to get quite a reception when they get back. Oh yes, I'll be so glad.\"},\n",
    "    {\"start\": 20.0, \"end\": 40.0, \"text\": \"I think that's pretty well fact, because they've landed so many safely now that I feel relieved. Just getting off of the moon was the thing that was... Have they met with the one that was circling the moon? Yes, they've round the moon.\"},\n",
    "    {\"start\": 40.0, \"end\": 60.0, \"text\": \"So I understand. That wasn't shown either, so I... But they say they have rendezvoused, so... That's a matter of making the circles and then coming down. What do you sort of imagine for the future? Do you imagine them standing up? I think they will. I think they will.\"},\n",
    "    {\"start\": 60.0, \"end\": 80.0, \"text\": \"We'll do some more exploring up there. Very positive. Because that was such a very small area, when you think of it, that they just gathered rocks and samples of soil and all. And they did a probe.\"},\n",
    "    {\"start\": 80.0, \"end\": 86.2, \"text\": \"For more information, visit www.FEMA.gov\"}\n",
    "]\n",
    "\n",
    "# Output folder\n",
    "output_dir = Path(\"transcriptions\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 1. Export human-readable text file\n",
    "txt_file = output_dir / \"transcription.txt\"\n",
    "with open(txt_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for chunk in transcriptions:\n",
    "        f.write(f\"[{chunk['start']:.1f}s - {chunk['end']:.1f}s]: {chunk['text']}\\n\\n\")\n",
    "print(f\"Text file exported to {txt_file}\")\n",
    "\n",
    "# 2. Export SRT file\n",
    "def format_srt_time(seconds):\n",
    "    h = int(seconds // 3600)\n",
    "    m = int((seconds % 3600) // 60)\n",
    "    s = int(seconds % 60)\n",
    "    ms = int((seconds - int(seconds)) * 1000)\n",
    "    return f\"{h:02}:{m:02}:{s:02},{ms:03}\"\n",
    "\n",
    "srt_file = output_dir / \"transcription.srt\"\n",
    "with open(srt_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, chunk in enumerate(transcriptions, start=1):\n",
    "        start_time = format_srt_time(chunk['start'])\n",
    "        end_time = format_srt_time(chunk['end'])\n",
    "        f.write(f\"{i}\\n{start_time} --> {end_time}\\n{chunk['text']}\\n\\n\")\n",
    "print(f\"SRT file exported to {srt_file}\")\n",
    "\n",
    "# 3. Export JSON file\n",
    "json_file = output_dir / \"transcription.json\"\n",
    "with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(transcriptions, f, indent=4)\n",
    "print(f\"JSON file exported to {json_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack-labs-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
